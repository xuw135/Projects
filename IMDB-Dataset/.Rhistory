#College.train = College[train, ]
#College.test = College[test, ]
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#train.size = dim(College)[1] / 2
#train = sample(1:dim(College)[1], train.size)
#test = -train
#College.train = College[train, ]
#College.test = College[test, ]
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#train.size = dim(College)[1] / 2
#train = sample(1:dim(College)[1], train.size)
#test = -train
#College.train = College[train, ]
#College.test = College[test, ]
lm.fit = lm(Apps~.-Apps, data=College[train,])
lm.pred = predict(lm.fit, College[-train,])
mean((lm.pred-College$Apps[-train])^2)
lm.fit = lm(Apps~.-Apps, data=College[train,])
lm.pred = predict(lm.fit, College[-train,])
mean((lm.pred-College$Apps[-train])^2)
install.packages("glmnet")
library(glmnet)
install.packages("glmnet")
knitr::opts_chunk$set(echo = TRUE)
x=model.matrix(Apps~.,College)[,-1]
library(ISLR)
attach(College)
View(College)
any(is.na(College))
dim(College)
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#train.size = dim(College)[1] / 2
#train = sample(1:dim(College)[1], train.size)
#test = -train
#College.train = College[train, ]
#College.test = College[test, ]
lm.fit = lm(Apps~.-Apps, data=College[train,])
lm.pred = predict(lm.fit, College[-train,])
mean((lm.pred-College$Apps[-train])^2)
install.packages("glmnet")
library(glmnet)
x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((ridge.pred - y[-train])^2)
#test MSE=
x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
ridge.pred
mean((ridge.pred - y[-train])^2)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid,thresh=1e-12)
#cross-validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
library(ISLR)
attach(College)
View(College)
any(is.na(College))
dim(College)
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#train.size = dim(College)[1] / 2
#train = sample(1:dim(College)[1], train.size)
#test = -train
#College.train = College[train, ]
#College.test = College[test, ]
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#split the data set into training and test set
lm.fit = lm(Apps~.-Apps, data=College[train,])
lm.pred = predict(lm.fit, College[-train,])
mean((lm.pred-College$Apps[-train])^2)
install.packages("glmnet")
library(glmnet)
install.packages("glmnet")
knitr::opts_chunk$set(echo = TRUE)
x=model.matrix(Apps~.,College)[,-1]
attach(College)
library(ISLR)
attach(College)
View(College)
any(is.na(College))
dim(College)
attach(College)
x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
#train model
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#split the data set into training and test set
lm.fit = lm(Apps~.-Apps, data=College[train,])
lm.pred = predict(lm.fit, College[-train,])
mean((lm.pred-College$Apps[-train])^2)
#test RSS=1355557
install.packages("glmnet")
library(glmnet)
attach(College)
x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
#train model
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
#cross validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
#best lambda=
#prediction
ridge.pred=predict(ridge.mod,s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
ridge.pred
mean((ridge.pred - y[-train])^2)
#test MSE= 2214482
#training model
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid,thresh=1e-12)
#cross-validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
#best lambda = 1.827875
#prediction
lasso.pred=predict(lasso.mod, s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((lasso.pred - y[-train])^2)
#coefficient
lasso.coef=predict(lasso.fit,type="coefficients",s=bestlam)[-1,]
#training model
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid,thresh=1e-12)
#cross-validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
#best lambda = 1.827875
#prediction
lasso.pred=predict(lasso.mod, s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((lasso.pred - y[-train])^2)
#coefficient
lasso.coef=predict(lasso.fit,type="coefficients",s=bestlam)[-1,]
#training model
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid,thresh=1e-12)
#cross-validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
#best lambda = 1.827875
#prediction
lasso.pred=predict(lasso.mod, s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((lasso.pred - y[-train])^2)
#coefficient
lasso.coef=predict(lasso.fit,type="coefficients",s=bestlam)[-1,]
#training model
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid,thresh=1e-12)
#cross-validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
#best lambda = 1.827875
#prediction
lasso.pred=predict(lasso.mod, s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((lasso.pred - y[-train])^2)
#coefficient
lasso.fit = glmnet(x[train,],y[train],alpha = 1,lambda = bestlam, thresh=1e-12)
lasso.coef=predict(lasso.fit,type="coefficients",s=bestlam)[-1,]
#non-zero coefficients
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0])
#
library (ISLR)
View(Hitters)
help(Hitters)
names(Hitters)
sum(is.na(Hitters$Salary))
#count missing values in salary
Hitters = na.omit(Hitters)
#remove the rows with missing data, and store it back in Hitters
#now 263 rows, original 322rows
dim(Hitters)
sum(is.na(Hitters$Salary))
#check again if there is any more missing values
x = model.matrix(Salary~.,Hitters)[,-1]
#store Hitters in matrix, and remove the intercept column the 1st column
#except salary column
x
y = Hitters$Salary
library(glmnet)
#provide function for building ridge regression and LASSO
grid = 10^seq(10,-2,length=100)
#create a sequence of 100 numbers between 10^10 and 10^(-2) at equal intervals
#use these numbers as lambda values when we construct ridge regression models
grid
ridge.mod = glmnet(x,y,alpha=0,lambda=grid)
#create ridge regression models
#alpha=0, ridge regression
#alpha=1, lasso model
#lambda=grid, tells glmnet function to construct ridge regression models with 100 values stored
dim(coef(ridge.mod))
#see how many coeffi stored in ridge.mod
ridge.mod$lambda[50]
grid[50]
coef(ridge.mod)[,50]
ridge.mod$lambda[60]
grid[60]
coef(ridge.mod)[,60]
predict(ridge.mod,s= 87.5,type ="coefficients")[1:20,] ##lambda = 100
ridge.mod$lambda[50]
#reach 50 coeffs
grid[50]
#if the same values stored in the vector grid
coef(ridge.mod)[,50]
ridge.mod$lambda[60]
grid[60]
coef(ridge.mod)[,60]
ridge.mod$lambda[60]
#same
grid[60]
coef(ridge.mod)[,60]
ridge.mod$lambda[50]
#reach 50 coeffs
grid[50]
#the 50th lambda values stored in the vector grid
#to see if they are same
#y-intercept = 407.3561
coef(ridge.mod)[,50]
ridge.mod$lambda[60]
#the 60th lambda value stored in ridge.mod
grid[60]
coef(ridge.mod)[,60]
#coeffis for the 60th lambda
predict(ridge.mod,s= 87.5,type ="coefficients")[1:20,] ##lambda = 100
#s=87.5 means lambda=87.5
predict(ridge.mod,s= 87.5,type ="coefficients")[1:20,] ##lambda = 100
set.seed(1)
#get the same result everytime run the code
train = sample(1:nrow(x), nrow(x)/2) ##default replace = FALSE
#??????
test = (-train)
y.test = y[test]
#remove the elements that relates to train vector from y, and store the remaining y in y.test
set.seed(1)
#get the same result everytime run the code
train = sample(1:nrow(x), nrow(x)/2) ##default replace = FALSE
#??????
test = (-train)
y.test = y[test]
#remove the elements that relates to train vector from y, and store the remaining y in y.test
ridge.mod = glmnet(x[train,],y[train],alpha = 0,lambda = grid,thresh = 1e-12)
## thresh is the threshold for the convergence of the optimization problem.
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])## lambda = 4
mean((ridge.pred-y.test)^2)
ridge.mod = glmnet(x[train,],y[train],alpha = 0,lambda = grid,thresh = 1e-12)
## thresh is the threshold for the convergence of the optimization problem.
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])## lambda = 4
mean((ridge.pred-y.test)^2)
#calculate test MSE
mean((mean(y[train])-y.test)^2) ## Null Model
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) ## Ridge Regression with only intercept
ridge.pred=predict(ridge.mod,s=0,x =x[train,], y = y[train], newx=x[test,],exact = T)
mean((ridge.pred -y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,x =x[train,], y = y[train],exact=T,type="coefficients")[1:20,]
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train], alpha=0) ##default is 10-fold cross-validation.
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out = glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.mod = glmnet(x[train,],y[train],alpha = 1,lambda = grid)
plot(lasso.mod)
set.seed(1)
cv.out = cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef = predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
library(ISLR)
attach(College)
any(is.na(College))
dim(College)
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#split the data set into training and test set
lm.fit = lm(Apps~.-Apps, data=College[train,])
lm.pred = predict(lm.fit, College[-train,])
mean((lm.pred-College$Apps[-train])^2)
#test RSS=1355557
install.packages("glmnet")
library(glmnet)
install.packages("glmnet")
attach(College)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
library(ISLR)
attach(College)
any(is.na(College))
dim(College)
set.seed(100)
train = sample(nrow(College),nrow(College)/2,replace = FALSE)
test = -train
#split the data set into training and test set
attach(College)
x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
#train model
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
#cross validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
#best lambda=342.5105
#prediction
ridge.pred=predict(ridge.mod,s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
ridge.pred
mean((ridge.pred - y[-train])^2)
#test MSE= 2214482
attach(College)
x=model.matrix(Apps~.,College)[,-1]
y=College$Apps
#train model
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
#cross validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
ridge.pred
mean((ridge.pred - y[-train])^2)
#prediction
ridge.pred=predict(ridge.mod,s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((ridge.pred - y[-train])^2)
#test MSE= 2214482
#training model
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid,thresh=1e-12)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod, s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((lasso.pred - y[-train])^2)
#test MSE =
#training model
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid,thresh=1e-12)
#cross-validation
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
#best lambda = 1.827875
#lasso prediction
lasso.pred=predict(lasso.mod, s=bestlam,
x=x[train,],y=y[train],newx=x[-train,],exact=T)
mean((lasso.pred - y[-train])^2)
#test MSE =
#coefficient
lasso.fit = glmnet(x[train,],y[train],alpha = 1,lambda = bestlam, thresh=1e-12)
lasso.coef=predict(lasso.fit,type="coefficients",s=bestlam)[-1,]
#non-zero coefficients
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0])
#there are 17 non-zero coefficient estimates
rocket <- read.csv("rocket.csv")
#View(rocket)
ggplot(data=rocket, aes(x = tot_impr, y = converted)) +
geom_point(alpha = 0.3, position = position_jitter(height = 0)) +
labs(x = "Total Impressions", y = "Converted")
data <- read.csv("data_hw4.csv")
head(data)
nrow(data)
View(data)
library(tidyverse)
library(fastDummies)
library(margins)
library(neuralnet) #neural net
library(rpart) #cart decision tree
library(ranger) #random forest
library(caret) #very useful tuning tool for machine learning models
library(MLmetrics) # compute performance measures such as AUC and MSE
library(plotROC) # ggplot for ROC/Gains
#library(kableExtra)
library(ggplot2)
install.packages("pROC")
library(pROC)
library(margins)
library(knitr)
install.packages("pROC")
data <- read.csv("data_hw4.csv")
head(data)
nrow(data)
#View(data)
#ITT
summary(lm(turnout~treatment, data=data))
alpha <- mean(data$contact[data$treatment==1])
alpha
summary(lm(turnout ~ treatment, data=data))$coefficients[2,1:2]
summary(lm(turnout ~ treatment, data=data))$coefficients[2,1:2] / alpha
install.packages("recommenderlab")
install.packages("recommenderlab")
install.packages("tidyverse")
install.packages("reshape2")
install.packages("tidyverse")
library(recommenderlab)
library(ggplot2)
library(ggplot2)
library(data.table)
library(reshape2)
library(ggplot2)
library(ggplot2)
library(data.table)
library(reshape2)
library(ggplot2)
install.packages("recommenderlab")
install.packages("tidyverse")
library(recommenderlab)
library("recommenderlab")
.libPaths()
library("recommenderlab")
.libPaths(recommenderlab)
.libPaths("recommenderlab")
install.packages("recommenderlab")
library(recommenderlab)
library("recommenderlab")
.libPaths()
.libPaths()
getwd()
.libPaths()
library("recommenderlab")
setwd("~/Documents/Projects/IMDB-Dataset")
getwd()
.libPaths()
.libPaths(/Library/Frameworks/R.framework/Versions/4.0/Resources/library)
.libPaths("/Library/Frameworks/R.framework/Versions/4.0/Resources/library")
.libPaths()
install.packages("recommenderlab")
library("recommenderlab")
.libPaths()
.libPaths()
install.packages("recommenderlab")
install.packages("recommenderlab")
install.packages("recommenderlab", dependencies = FALSE)
myPaths <- .libPaths()   # get the paths
myPaths <- .libPaths()   # get the paths
myPaths <- c(myPaths[2], myPaths[1])  # switch them
.libPaths(myPaths)  # reassign them
.libPaths("/Library/Frameworks/R.framework/Versions/4.0/Resources/library")
.libPaths()
myPaths <- .libPaths()   # get the paths
myPaths <- c(myPaths[2], myPaths[1])  # switch them
.libPaths(myPaths)  # reassign them
